{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from denoising_diffusion_pytorch import UnetLCAtten, Trainer, GaussianDiffusion\n",
    "from collections import namedtuple\n",
    "\n",
    "from einops import rearrange, reduce\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from functools import partial\n",
    "import tqdm\n",
    "from torchvision import transforms as T, utils\n",
    "\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ModelPrediction =  namedtuple('ModelPrediction', ['pred_noise', 'pred_x_start'])\n",
    "\n",
    "# helpers functions\n",
    "\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if callable(d) else d\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    \"\"\"\n",
    "    linear schedule, proposed in original ddpm paper\n",
    "    \"\"\"\n",
    "    scale = 1000 / timesteps\n",
    "    beta_start = scale * 0.0001\n",
    "    beta_end = scale * 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps, dtype = torch.float64)\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s = 0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule\n",
    "    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    t = torch.linspace(0, timesteps, steps, dtype = torch.float64) / timesteps\n",
    "    alphas_cumprod = torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "def sigmoid_beta_schedule(timesteps, start = -3, end = 3, tau = 1, clamp_min = 1e-5):\n",
    "    \"\"\"\n",
    "    sigmoid schedule\n",
    "    proposed in https://arxiv.org/abs/2212.11972 - Figure 8\n",
    "    better for images > 64x64, when used during training\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    t = torch.linspace(0, timesteps, steps, dtype = torch.float64) / timesteps\n",
    "    v_start = torch.tensor(start / tau).sigmoid()\n",
    "    v_end = torch.tensor(end / tau).sigmoid()\n",
    "    alphas_cumprod = (-((t * (end - start) + start) / tau).sigmoid() + v_end) / (v_end - v_start)\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "def normalize_to_neg_one_to_one(img):\n",
    "    return img * 2 - 1\n",
    "\n",
    "def unnormalize_to_zero_to_one(t):\n",
    "    return (t + 1) * 0.5\n",
    "\n",
    "def identity(t, *args, **kwargs):\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiffusionContinuousT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: UnetLCAtten,\n",
    "        *,\n",
    "        image_size,\n",
    "        timesteps = 1000,\n",
    "        sampling_timesteps = None,\n",
    "        loss_type = 'l1',\n",
    "        objective = 'pred_noise',\n",
    "        beta_schedule = 'sigmoid',\n",
    "        schedule_fn_kwargs = dict(),\n",
    "        p2_loss_weight_gamma = 0., # p2 loss weight, from https://arxiv.org/abs/2204.00227 - 0 is equivalent to weight of 1 across time - 1. is recommended\n",
    "        p2_loss_weight_k = 1,\n",
    "        ddim_sampling_eta = 0.,\n",
    "        auto_normalize = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert not (type(self) == GaussianDiffusionContinuousT and model.channels != model.out_dim)\n",
    "        assert not model.random_or_learned_sinusoidal_cond\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        self.channels = self.model.channels\n",
    "        self.self_condition = self.model.self_condition\n",
    "\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.objective = objective\n",
    "\n",
    "        assert objective in {'pred_noise', 'pred_x0', 'pred_v'}, 'objective must be either pred_noise (predict noise) or pred_x0 (predict image start) or pred_v (predict v [v-parameterization as defined in appendix D of progressive distillation paper, used in imagen-video successfully])'\n",
    "\n",
    "        if beta_schedule == 'linear':\n",
    "            beta_schedule_fn = linear_beta_schedule\n",
    "        elif beta_schedule == 'cosine':\n",
    "            beta_schedule_fn = cosine_beta_schedule\n",
    "        elif beta_schedule == 'sigmoid':\n",
    "            beta_schedule_fn = sigmoid_beta_schedule\n",
    "        else:\n",
    "            raise ValueError(f'unknown beta schedule {beta_schedule}')\n",
    "\n",
    "        betas = beta_schedule_fn(timesteps, **schedule_fn_kwargs)\n",
    "\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)\n",
    "\n",
    "        timesteps, = betas.shape\n",
    "        self.num_timesteps = int(timesteps)\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "        # sampling related parameters\n",
    "\n",
    "        self.sampling_timesteps = default(sampling_timesteps, timesteps) # default num sampling timesteps to number of timesteps at training\n",
    "\n",
    "        assert self.sampling_timesteps <= timesteps\n",
    "        self.is_ddim_sampling = self.sampling_timesteps < timesteps\n",
    "        self.ddim_sampling_eta = ddim_sampling_eta\n",
    "\n",
    "        # helper function to register buffer from float64 to float32\n",
    "\n",
    "        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))\n",
    "\n",
    "        register_buffer('betas', betas)\n",
    "        register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "\n",
    "        register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n",
    "        register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n",
    "        register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n",
    "        register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "\n",
    "        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n",
    "\n",
    "        register_buffer('posterior_variance', posterior_variance)\n",
    "\n",
    "        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n",
    "\n",
    "        register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min =1e-20)))\n",
    "        register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))\n",
    "        register_buffer('posterior_mean_coef2', (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))\n",
    "\n",
    "        # calculate p2 reweighting\n",
    "\n",
    "        register_buffer('p2_loss_weight', (p2_loss_weight_k + alphas_cumprod / (1 - alphas_cumprod)) ** -p2_loss_weight_gamma)\n",
    "\n",
    "        # auto-normalization of data [0, 1] -> [-1, 1] - can turn off by setting it to be False\n",
    "\n",
    "        self.normalize = normalize_to_neg_one_to_one if auto_normalize else identity\n",
    "        self.unnormalize = unnormalize_to_zero_to_one if auto_normalize else identity\n",
    "        \n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.model.parameters()).device\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        alpha_t, sigma_t = self.alpha_t_sigma_t(t)\n",
    "        return (x_t - sigma_t*noise) / alpha_t\n",
    "        return (\n",
    "            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "\n",
    "    def predict_noise_from_start(self, x_t, t, x0):\n",
    "        alpha_t, sigma_t = self.alpha_t_sigma_t(t)\n",
    "        return (x_t - alpha_t*x0) / sigma_t\n",
    "        return (\n",
    "            (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0) / \\\n",
    "            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "        )\n",
    "\n",
    "    # def predict_v(self, x_start, t, noise):\n",
    "    #     return (\n",
    "    #         extract(self.sqrt_alphas_cumprod, t, x_start.shape) * noise -\n",
    "    #         extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * x_start\n",
    "    #     )\n",
    "\n",
    "    # def predict_start_from_v(self, x_t, t, v):\n",
    "    #     return (\n",
    "    #         extract(self.sqrt_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "    #         extract(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * v\n",
    "    #     )\n",
    "\n",
    "    def q_posterior(self, x_start, x_t, t):\n",
    "        posterior_mean = (\n",
    "            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n",
    "            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def model_predictions(self, x, t, x_self_cond = None, clip_x_start = False, rederive_pred_noise = False):\n",
    "        model_output = self.model(x, t, x_self_cond)\n",
    "        maybe_clip = partial(torch.clamp, min = -1., max = 1.) if clip_x_start else identity\n",
    "\n",
    "        if self.objective == 'pred_noise':\n",
    "            pred_noise = model_output\n",
    "            x_start = self.predict_start_from_noise(x, t, pred_noise)\n",
    "            x_start = maybe_clip(x_start)\n",
    "\n",
    "            if clip_x_start and rederive_pred_noise:\n",
    "                pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        elif self.objective == 'pred_x0':\n",
    "            x_start = model_output\n",
    "            x_start = maybe_clip(x_start)\n",
    "            pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        # elif self.objective == 'pred_v':\n",
    "        #     v = model_output\n",
    "        #     x_start = self.predict_start_from_v(x, t, v)\n",
    "        #     x_start = maybe_clip(x_start)\n",
    "        #     pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        return ModelPrediction(pred_noise, x_start)\n",
    "\n",
    "    def p_mean_variance(self, x, t, x_self_cond = None, clip_denoised = True):\n",
    "        preds = self.model_predictions(x, t, x_self_cond)\n",
    "        x_start = preds.pred_x_start\n",
    "\n",
    "        if clip_denoised:\n",
    "            x_start.clamp_(-1., 1.)\n",
    "\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start = x_start, x_t = x, t = t)\n",
    "        return model_mean, posterior_variance, posterior_log_variance, x_start\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t: int, x_self_cond = None):\n",
    "        b, *_, device = *x.shape, x.device\n",
    "        batched_times = torch.full((b,), t, device = x.device, dtype = torch.long)\n",
    "        model_mean, _, model_log_variance, x_start = self.p_mean_variance(x = x, t = batched_times, x_self_cond = x_self_cond, clip_denoised = True)\n",
    "        noise = torch.randn_like(x) if t > 0 else 0. # no noise if t == 0\n",
    "        pred_img = model_mean + (0.5 * model_log_variance).exp() * noise\n",
    "        return pred_img, x_start\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, shape, return_all_timesteps = False):\n",
    "        batch, device = shape[0], self.betas.device\n",
    "\n",
    "        img = torch.randn(shape, device = device)\n",
    "        imgs = [img]\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for t in tqdm(reversed(range(0, self.num_timesteps)), desc = 'sampling loop time step', total = self.num_timesteps):\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            img, x_start = self.p_sample(img, t, self_cond)\n",
    "            imgs.append(img)\n",
    "\n",
    "        ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n",
    "\n",
    "        ret = self.unnormalize(ret)\n",
    "        return ret\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_sample(self, shape, return_all_timesteps = False):\n",
    "        batch, device, total_timesteps, sampling_timesteps, eta, objective = shape[0], self.betas.device, self.num_timesteps, self.sampling_timesteps, self.ddim_sampling_eta, self.objective\n",
    "\n",
    "        times = torch.linspace(-1, total_timesteps - 1, steps = sampling_timesteps + 1) / total_timesteps   # [-1, 0, 1, 2, ..., T-1] when sampling_timesteps == total_timesteps\n",
    "        times = list(reversed(times.tolist()))\n",
    "        time_pairs = list(zip(times[:-1], times[1:])) # [(T-1, T-2), (T-2, T-3), ..., (1, 0), (0, -1)]\n",
    "\n",
    "        img = torch.randn(shape, device = device)\n",
    "        imgs = [img]\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for time, time_next in tqdm(time_pairs, desc = 'sampling loop time step'):\n",
    "            time_cond = torch.full((batch,), time, device = device)\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            pred_noise, x_start, *_ = self.model_predictions(img, time_cond, self_cond, clip_x_start = True, rederive_pred_noise = True)\n",
    "\n",
    "            if time_next < 0:\n",
    "                img = x_start\n",
    "                imgs.append(img)\n",
    "                continue\n",
    "            \n",
    "            alpha_next, sigma_next = self.alpha_t_sigma_t(torch.full((batch,), time_next, device = device))\n",
    "\n",
    "            img = x_start * alpha_next + sigma_next * pred_noise\n",
    "                  \n",
    "            imgs.append(img)\n",
    "\n",
    "        ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n",
    "\n",
    "        ret = self.unnormalize(ret)\n",
    "        return ret\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size = 16, return_all_timesteps = False):\n",
    "        image_size, channels = self.image_size, self.channels\n",
    "        sample_fn = self.p_sample_loop if not self.is_ddim_sampling else self.ddim_sample\n",
    "        return sample_fn((batch_size, channels, image_size, image_size), return_all_timesteps = return_all_timesteps)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def interpolate(self, x1, x2, t = None, lam = 0.5):\n",
    "        b, *_, device = *x1.shape, x1.device\n",
    "        t = default(t, self.num_timesteps - 1)\n",
    "\n",
    "        assert x1.shape == x2.shape\n",
    "\n",
    "        t_batched = torch.full((b,), t, device = device)\n",
    "        xt1, xt2 = map(lambda x: self.q_sample(x, t = t_batched), (x1, x2))\n",
    "\n",
    "        img = (1 - lam) * xt1 + lam * xt2\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for i in tqdm(reversed(range(0, t)), desc = 'interpolation sample time step', total = t):\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            img, x_start = self.p_sample(img, i, self_cond)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def loss_fn(self):\n",
    "        if self.loss_type == 'l1':\n",
    "            return F.l1_loss\n",
    "        elif self.loss_type == 'l2':\n",
    "            return F.mse_loss\n",
    "        else:\n",
    "            raise ValueError(f'invalid loss type {self.loss_type}')\n",
    "\n",
    "    def random_times(self, batch_size):\n",
    "        # times are now uniform from 0 to 1\n",
    "        return torch.zeros((batch_size,), device = self.device).float().uniform_(0, 1)\n",
    "\n",
    "    def alpha_t_sigma_t(self, t, view=(-1,1,1,1), s=0.008):\n",
    "        alpha_t = torch.cos((t+s)/(1+s)*math.pi*0.5)\n",
    "        sigma_t = torch.sqrt(1 - alpha_t**2 )\n",
    "\n",
    "        return alpha_t.view(*view), sigma_t.view(*view)\n",
    "\n",
    "    def p_losses(self, x_start, t, noise = None):\n",
    "        b, c, h, w = x_start.shape\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        # noise sample\n",
    "        # sqrt_alphas_cumprod\n",
    "        alpha_t, sigma_t = self.alpha_t_sigma_t(t)\n",
    "\n",
    "        # x = self.q_sample(x_start = x_start, t = t, noise = noise)\n",
    "        x = alpha_t * x_start + sigma_t * noise\n",
    "\n",
    "        # if doing self-conditioning, 50% of the time, predict x_start from current set of times\n",
    "        # and condition with unet with that\n",
    "        # this technique will slow down training by 25%, but seems to lower FID significantly\n",
    "\n",
    "        # x_self_cond = None\n",
    "        # if self.self_condition and random() < 0.5:\n",
    "        #     with torch.no_grad():\n",
    "        #         x_self_cond = self.model_predictions(x, t).pred_x_start\n",
    "        #         x_self_cond.detach_()\n",
    "\n",
    "        # predict and take gradient step\n",
    "\n",
    "        # model_out = self.model(x, t, x_self_cond)\n",
    "        model_out = self.model(x, t)\n",
    "\n",
    "        if self.objective == 'pred_noise':\n",
    "            target = noise\n",
    "        elif self.objective == 'pred_x0':\n",
    "            target = x_start\n",
    "        # elif self.objective == 'pred_v':\n",
    "        #     v = self.predict_v(x_start, t, noise)\n",
    "        #     target = v\n",
    "        else:\n",
    "            raise ValueError(f'unknown objective {self.objective}')\n",
    "\n",
    "        loss = self.loss_fn(model_out, target, reduction = 'none')\n",
    "        loss = reduce(loss, 'b ... -> b (...)', 'mean')\n",
    "\n",
    "        # loss = loss * extract(self.p2_loss_weight, t, loss.shape)\n",
    "        return loss.mean()\n",
    "\n",
    "    def forward(self, img, *args, **kwargs):\n",
    "        b, c, h, w, device, img_size, = *img.shape, img.device, self.image_size\n",
    "        assert h == img_size and w == img_size, f'height and width of image must be {img_size}'\n",
    "        # t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n",
    "        \n",
    "        t = self.random_times(b)\n",
    "        img = self.normalize(img)\n",
    "        return self.p_losses(img, t, *args, **kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "lca_atten_continous_dict = torch.load(\"LCAtten_cos_continoust_results/model-155.pt\")\n",
    "cos_baseline = torch.load(\"cos_results/model-155.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 500/500 [00:13<00:00, 36.24it/s]\n"
     ]
    }
   ],
   "source": [
    "model = UnetLCAtten(\n",
    "    dim = 64,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    max_t_step = 1.,\n",
    ").cuda()\n",
    "# model = torch.compile(model)\n",
    "\n",
    "diffusion = GaussianDiffusionContinuousT(\n",
    "    model,\n",
    "    image_size = 32,\n",
    "    timesteps = 1000,           # number of steps\n",
    "    sampling_timesteps = 500,   # number of sampling timesteps (using ddim for faster inference [see citation for ddim paper])\n",
    "    loss_type = 'l1',            # L1 or L2\n",
    "    beta_schedule = 'cosine',\n",
    ").cuda()\n",
    "\n",
    "diffusion.load_state_dict(lca_atten_continous_dict['model'])\n",
    "\n",
    "img = diffusion.sample(1, True)\n",
    "\n",
    "utils.save_image(img[0], 'test.png', nrow = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 100/100 [00:02<00:00, 36.23it/s]\n"
     ]
    }
   ],
   "source": [
    "model = UnetLCAtten(\n",
    "    dim = 64,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    ").cuda()\n",
    "# model = torch.compile(model)\n",
    "\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size = 32,\n",
    "    timesteps = 1000,           # number of steps\n",
    "    sampling_timesteps = 100,   # number of sampling timesteps (using ddim for faster inference [see citation for ddim paper])\n",
    "    loss_type = 'l1',            # L1 or L2\n",
    "    beta_schedule = 'cosine',\n",
    ").cuda()\n",
    "\n",
    "diffusion.load_state_dict(cos_baseline['model'])\n",
    "\n",
    "img = diffusion.sample(1, True)\n",
    "\n",
    "utils.save_image(img[0], 'test.png', nrow = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 250/250 [00:38<00:00,  6.49it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/mnt/hdd/joshua/workspace/ddpm/test.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.158.24/mnt/hdd/joshua/workspace/ddpm/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m diffusion\u001b[39m.\u001b[39mload_state_dict(lca_atten_continous_dict[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.158.24/mnt/hdd/joshua/workspace/ddpm/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m img \u001b[39m=\u001b[39m diffusion\u001b[39m.\u001b[39msample(\u001b[39m1\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B143.248.158.24/mnt/hdd/joshua/workspace/ddpm/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m utils\u001b[39m.\u001b[39;49msave_image(img, \u001b[39m'\u001b[39;49m\u001b[39mtest.png\u001b[39;49m\u001b[39m'\u001b[39;49m, nrow \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ddpm/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ddpm/lib/python3.9/site-packages/torchvision/utils.py:149\u001b[0m, in \u001b[0;36msave_image\u001b[0;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m grid \u001b[39m=\u001b[39m make_grid(tensor, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    148\u001b[0m \u001b[39m# Add 0.5 after unnormalizing to [0, 255] to round to the nearest integer\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m ndarr \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39;49mmul(\u001b[39m255\u001b[39;49m)\u001b[39m.\u001b[39;49madd_(\u001b[39m0.5\u001b[39;49m)\u001b[39m.\u001b[39;49mclamp_(\u001b[39m0\u001b[39;49m, \u001b[39m255\u001b[39;49m)\u001b[39m.\u001b[39;49mpermute(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39muint8)\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    150\u001b[0m im \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(ndarr)\n\u001b[1;32m    151\u001b[0m im\u001b[39m.\u001b[39msave(fp, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39mformat\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "diffusion.load_state_dict(lca_atten_continous_dict['model'])\n",
    "\n",
    "img = diffusion.sample(1, True)\n",
    "\n",
    "utils.save_image(img[0], 'test.png', nrow = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_image(img[0], 'test.png', nrow = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddpm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
